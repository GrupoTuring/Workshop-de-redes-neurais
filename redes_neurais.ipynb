{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('torch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a5cd74ba85a3b6a037c59ac3f3634fcdd9437555c9fe253dd51f04000fcd493e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Workshop Redes Neurais\n",
    "## Grupo Turing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Proposta de roteiro para o nb (deletar depois)\n",
    "\n",
    "#### O que deve ser falado antes do nb?\n",
    "- Introdução à redes neurais\n",
    "- estrutura geral de uma rede neural (talvez falar que é parecido com uma regressão logística?)\n",
    "- computation graph (talvez?)\n",
    "- forward propagation\n",
    "- back propagation (se falar de fp acho que é a sequência lógica)\n",
    "- optimizaçaõ e gradient descent\n",
    "\n",
    "#### Conteúdos do nb\n",
    "- Básico de Pytorch\n",
    "  - comparação com np\n",
    "  - operações básicas\n",
    "  - Variables (falar de back propagation?)\n",
    "- Implementando uma NN\n",
    "  - Implementar uma regressão logística (?)\n",
    "  - Implementar NN\n",
    "  - Falar sobre CNN e LSTM (?)\n",
    "  \n",
    "**Obs: ** Ainda é preciso deixar o NB mais bonito, mais imagens e talvez melhorar a estrura dos exemplos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Básicos de Pytorch\n",
    "\n",
    "Primeiro vamos ver alguns análogos entre **numpy** e **Pytorch**\n",
    "\n",
    "### Matrizes\n",
    " - Em Pytorch, matrizes (*arrays*) são chamados de tensores.\n",
    " - Uma matriz $3\\times3$, por exemplo é um tensor $3\\times3$\n",
    " - Podemos criar um array numpy com o método `np.numpy()`\n",
    " - Podemos pegar o tipo do array com `type()`\n",
    " - Podemos pegar o formato do *array* com `np.shape()`. Linha $\\times$ Coluna"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "array = [[1,2,3],[4,5,6]]\n",
    "primeiro_array = np.array(array) # array 2x3\n",
    "print(f\"Array do tipo: {type(primeiro_array)}\")\n",
    "print(f\"Array de formato: {np.shape(primeiro_array)}\")\n",
    "print(primeiro_array)"
   ]
  },
  {
   "source": [
    "- Criamos um tensor com o método `torch.Tensor()`\n",
    "- `tensor.type`: tipo do *array*, nesse caso um tensor\n",
    "- `tensor.shape`: formato do *array*. Linha $\\times$ Coluna "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.Tensor(array)\n",
    "print(f\"Array do tipo: {tensor.type}\")\n",
    "print(f\"Array de formato: {tensor.shape}\")\n",
    "print(tensor)"
   ]
  },
  {
   "source": [
    "Podemos fazer a alocação de *arrays* de maneira análoga nas duas linguagens:\n",
    " - `np.ones()` = `torch.ones()`\n",
    " - `np.random.rand()` = `torch.rand()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numpy:\\n {np.ones((2,3))}\\n\")\n",
    "\n",
    "print(torch.ones((2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Numpy:\\n {np.random.rand(2,3)}\\n\")\n",
    "\n",
    "print(torch.rand(2,3))"
   ]
  },
  {
   "source": [
    "### Convertendo de numpy para torch e vice-versa\n",
    "\n",
    "Em muitos pontos **numpy** e **pytorch** são bem parecidos em suas estruturas, e muitas das vezes podemos utilizar os dois em conjunto. Assim normalmente convertemos resultados de redes neurais - que são tensores - para **arrays** de **numpy**.\n",
    "\n",
    "Os métodos para fazer a conversão entre tensores e arrays numpy:\n",
    " - `torch.from_numpy()`: de um array numpy para um tensore\n",
    " - `numpy()`: de um tensor para um array numpy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.random.rand(2,2)\n",
    "print(f\"{type(array)} \\n {array} \\n\")\n",
    "\n",
    "de_numpy_para_tensor = torch.from_numpy(array)\n",
    "print(f\"{de_numpy_para_tensor} \\n\")\n",
    "\n",
    "tensor = de_numpy_para_tensor\n",
    "de_tensor_para_numpy = tensor.numpy()\n",
    "print(f\"{type(de_tensor_para_numpy)} \\n {de_tensor_para_numpy}\")"
   ]
  },
  {
   "source": [
    "Quando fazemos estas conversões também podemos fazer um *typecast* (mudagem do tipo) das variáveis, isso pode ser útil já que o Pytorch faz uma série de computações de baixo nível, o qual o tipo primitivo das variáveis precisa ser bem especificado e definido, para isso podemos usar o método `tensor.type(torch.TipoDeTensor)`, alguns tipode de tensores nativos do Pytorch são:\n",
    "  - `torch.FloatTensor` - pontos flutuantes de 32-bits\n",
    "  - `torch.DoubleTensor` - pontos flutuantes de 64-bits\n",
    "  - `torch.IntTensor` - números inteiros de 32-bits\n",
    "  - `torch.LongTensor` - númeos inteiros de 64-bits\n",
    "É muito comum encontrarmos *bugs* causados pela utilização errada de algum tipo primitivo, você pode ler sobre todos eles na [documentação do Pytorch](https://pytorch.org/docs/stable/tensors.html)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([[1,10],[2,20]])\n",
    "\n",
    "# Transformar em um tensor de Floats:\n",
    "tensor_float = torch.from_numpy(array).type(torch.FloatTensor)\n",
    "print(f\"{type(tensor_float)} \\n {tensor_float}\\n\")\n",
    "\n",
    "# Transformar em um tensor de Longs:\n",
    "tensor_long = torch.from_numpy(array).type(torch.LongTensor)\n",
    "print(f\"{type(tensor_long)} \\n {tensor_long}\")"
   ]
  },
  {
   "source": [
    "### Matemática básica com Pytorch\n",
    "*considere a e b dois tensores*\n",
    "\n",
    "- Redefinir o tamanho: `view()`\n",
    "- Adição: `torch.add(a,b)` = a + b\n",
    "- Subtração: `torch.sub(a,b)` = a - b\n",
    "- Multiplicação elemento-a-elemento = `torch.mul(a,b)` = a * b\n",
    "- Divisão elemento-a-elemento = `torch.div(a,b)` = a / b\n",
    "- Média: `a.mean()`\n",
    "- Desvio Padrão (Standart Deviantion - std): `a.std()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.ones(3,3)\n",
    "print(\"\\n\", tensor, \"\\n\")\n",
    "\n",
    "print(f\"{tensor.view(9).shape}: {tensor.view(9)} \\n\")\n",
    "\n",
    "print(f\"Adição: \\n{torch.add(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Subtração: \\n{torch.sub(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Multiplicação elemento-a-elemento: \\n{torch.mul(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Divisão elemento-a-elemento: \\n{torch.div(tensor, tensor)} \\n\")\n",
    "\n",
    "tensor = torch.Tensor([1,2,3,4,5])\n",
    "print(f\"Média: {tensor.mean()} \\n\")\n",
    "\n",
    "print(f\"Desvio padrão: {tensor.std()} \\n\")"
   ]
  },
  {
   "source": [
    "### Variáveis\n",
    "\n",
    "- Acumulam os gradientes\n",
    "- Na rede neural utilizaremos pytorch. Como explicamos anteriormente nas, redes neurais os gradientes são calculados na *backpropagation*.\n",
    "- A diferença entre variáveis e tensores é a de que variáveis acumulam os gradientes\n",
    "- Também podemos fazer operações matemáticas com variáveis\n",
    "- Dessa maneira, se queremos fazer a *backpropagation* precisamos de variáveis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "var = Variable(torch.ones(3), requires_grad = True)\n",
    "var"
   ]
  },
  {
   "source": [
    "Vamos ver um exemplo de como as Variávies são utilizadas em uma *backpropagation*, com duas função $f(y) = \\sum y$, $y(x) = x^2$ e $x = (3,5)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [3,5]\n",
    "tensor = torch.Tensor(array)\n",
    "x = Variable(tensor, requires_grad = True)\n",
    "y = x**2\n",
    "print(f\" x = {x}\")\n",
    "\n",
    "f = sum(y)\n",
    "print(f\" f =  {f}\")\n",
    "\n",
    "f.backward() # Realiza as derivadas parciais\n",
    "\n",
    "print(f\"Gradientes: {x.grad}\")"
   ]
  },
  {
   "source": [
    "Vamos explicar passo a passo quais foram as operações feitas pelo Pytorch:\n",
    "- Primeiro ele recebe os elementos do tensor e faz a primeira operação com eles $y_1 = 3^2 = 9$ e $y_2 = 5^2 = 25$\n",
    "- Agora ele soma o tensor, retornando assim um único valor escalar: $\\sum_i y_i = y_1 + y_2 = 9 + 25 = 34$\n",
    "- O gradiente é a derivada parcial de cada elemento, ou seja o gradiente \"1\" é a derivada relativa à $y_1$ e o gradiente \"2\" é relativo à $y_2$ \n",
    "- derivada relativa à $y_1$ é $\\frac{\\partial}{\\partial y_1}(3^2) = 2*3 = 6$\n",
    "- derivada relativa à $y_2$ é $\\frac{\\partial}{\\partial y_2}(5^2) = 2*5 = 10$\n",
    "- Assim ficamos com os gradientes $(6, 10)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Exercícios\n",
    "\n",
    "Coplete às células de código abaixo no campo indicado por \"...\":"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Crie um tensor com base no *array* dado:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [[10,100,1000], [20,200,2000]]\n",
    "tensor = \"...\"\n",
    "tensor"
   ]
  },
  {
   "source": [
    "Crie um tensor de formato $(5,3)$ no qual todos os elementos são o número 1, depois utilize o método `.shape` para verificar seu formato:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_de_uns = \"...\"\n",
    "formato_do_tensor = \"...\"\n",
    "\n",
    "print(f\"Tensor: \\n {tensor_de_uns}\\n\")\n",
    "print(f\"Formato: {formato_do_tensor}\")"
   ]
  },
  {
   "source": [
    "Converta o *array* numpy para um tesnor de pytorch, depois transforme o tesnor em um *array* numpy novamente."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([[1,1,2,3], [5,8,13,21]])\n",
    "\n",
    "de_numpy_para_tensor = \"...\"\n",
    "print(f\"{de_numpy_para_tensor} \\n É um tensor? {isinstance(de_numpy_para_tensor, torch.Tensor)}\\n\")\n",
    "\n",
    "de_tensor_para_numpy = \"...\"\n",
    "print(f\"{de_tensor_para_numpy} \\n É um array numpy? {isinstance(de_tensor_para_numpy, np.ndarray)}\")"
   ]
  },
  {
   "source": [
    "Complete a célula abaixo com as operações indicadas:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_a = torch.Tensor([[5,8],[5,4]])\n",
    "tensor_b = torch.Tensor([[10,16],[10,8]])\n",
    "\n",
    "soma = \"...\" # a+b\n",
    "subtracao = \"...\" # b-a\n",
    "mul = \"...\" # a*b\n",
    "div = \"...\" # b/a\n",
    "media = \"...\" # media de a\n",
    "std = \"...\" # desvio padrão de b\n",
    "\n",
    "print(f\"Soma:\\n {soma} \\n\"\n",
    "      f\"Subtração:\\n {subtracao} \\n\"\n",
    "      f\"Multiplicação:\\n {mul} \\n\"\n",
    "      f\"Divisão:\\n {div} \\n\"\n",
    "      f\"Média: {media} \\n\"\n",
    "      f\"Desvio Padrão: {std} \\n\")"
   ]
  },
  {
   "source": [
    "Crie uma **Varíavel** do pytorch com o tensor definido. Depois defina as equações $y = log_e(x)$ e $f(y) = 2*media(y)$. Para então aplicar a *backpropagation* em $f(x)$ e calcular seus gradientes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [4,5]\n",
    "tensor = torch.Tensor(array)\n",
    "\n",
    "x = \"...\"\n",
    "print(f\" x = {x}\")\n",
    "\n",
    "y = \"...\" # Dica: use o operador torch.log()\n",
    "print(f\" y = {y}\")\n",
    "\n",
    "f = \"...\"\n",
    "print(f\" f = {f}\")\n",
    "\n",
    "# Escreva aqui a backpropagation de f\n",
    "...\n",
    "\n",
    "if isinstance(x, torch.Tensor): print(f\"Gradientes: {x.grad}\")\n",
    "else: print(\"Complete o exerćicio!\")"
   ]
  },
  {
   "source": [
    "## Implementando uma rede neural\n",
    "### Conhecendo e preparando nossos dados \n",
    "\n",
    "**[Fashion MNIST](https://www.kaggle.com/zalando-research/fashionmnist)** é uma coleção de diversas peças de roupas retiradas do serviço de *e-commerce* Zalando, ele consiste de cerca de 60.000 entradas de treino de 10.000 de teste. Cada entrada é uma imagem de 28x28 pixels em escala cinza. As peças de roupa estão classificadas da seguinte maneira:\n",
    "  - 0 *T-shirt*\n",
    "  - 1 *Trouser*\n",
    "  - 2 *Pullover*\n",
    "  - 3 *Dress*\n",
    "  - 4 *Coat*\n",
    "  - 5 *Sandal*\n",
    "  - 6 *Shirt*\n",
    "  - 7 *Sneaker*\n",
    "  - 8 *Bag*\n",
    "  - 9 *Ankle boot*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_inicial = pd.read_csv(\"fashion-mnist_train.csv\")\n",
    "df_inicial.head() # Como cada coluna representa o valor de cada pixel, a tabela dos dados não é muito \"emocionante\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validos = 10000\n",
    "n_treino = len(df_inicial) - n_validos\n",
    "print(f\"Número de entradas de treino: {n_treino}\\n\"\n",
    "      f\"Número de entradas de validação: {n_validos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos essa função para dividir entre dados de treino e validação\n",
    "def divide_valores(a,n):\n",
    "     return a[:n].copy(), a[n:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir dataset entre dados x e labels y\n",
    "y, x = df_inicial[\"label\"].values, df_inicial.loc[:, df_inicial.columns != \"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_treino, x_valido = divide_valores(x, n_treino)\n",
    "y_treino, y_valido = divide_valores(y, n_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Formato do x de treino: {x_treino.shape}\\n\"\n",
    "      f\"Formato do x de validação: {x_valido.shape}\\n\"\n",
    "      f\"Formato do y de treino: {y_treino.shape}\\n\"\n",
    "      f\"Formato do y de validação: {y_valido.shape}\")"
   ]
  },
  {
   "source": [
    "Uma etapa comum de pré processamento de dados em aprendizado por máquina é centralizar padronizar nosso *dataset*, o que isso basicamente significa é que iremos subtrair a média de todo o *dataset* e dividi-lo pelo seu desvio padrão. Esse processo ajuda a agilizar o processo de aprendizado."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "media = x_treino.mean()\n",
    "desvio_padrao = x_treino.std()\n",
    "\n",
    "x_treino = (x_treino-media)/desvio_padrao\n",
    "print(f\"Média antes do pré processamento: {media:.2f}\\n\"\n",
    "      f\"Desvio padrão antes do pré processamente: {desvio_padrao:.2f}\\n\"\n",
    "      f\"Média depois do pré processamento: {x_treino.mean():.2f}\\n\"\n",
    "      f\"Desvio padrão depois do pré processamento: {x_treino.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O mesmo deve ser feito com a validação\n",
    "\n",
    "x_valido = (x_valido-media)/desvio_padrao\n",
    "print(f\"Média pós processamento: {x_valido.mean():.2f}\\n\"\n",
    "      f\"Desvio padrão pós processamento: {x_valido.std():.2f}\")"
   ]
  },
  {
   "source": [
    "Vamos visualizar algumas das imagens de nosso *dataset*:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Essa função vai nos ajudar a visualizar as imagens\n",
    "def mostrar(img, title=None):\n",
    "    labels = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    if title is not None: plt.title(labels[int(title)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_imgs = np.reshape(x_valido, (-1, 28, 28))\n",
    "\n",
    "index = 100\n",
    "mostrar(x_imgs[index], y_valido[index])"
   ]
  },
  {
   "source": [
    "### Adaptando os dados para o Pytorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforme os respectivos datasets de numpy para tensores torch, \n",
    "# o tensor x_treino_torch e x_valido_torch devem ser do tipo torch.FloatTensor\n",
    "# Dica, lembre-se da seção \"Convertendo de numpy para torch e vice-versa\"\n",
    "x_treino_torch = \"...\"\n",
    "y_treino_torch = \"...\"\n",
    "\n",
    "x_valido_torch = \"...\"\n",
    "y_valido_torch = \"...\"\n",
    "\n",
    "tamanho_batch = 100\n",
    "n_iters = 10000\n",
    "\n",
    "n_epochs = int((n_iters / len(y_treino)) * tamanho_batch)\n",
    "\n",
    "# Transformamos em um dataset de tensores\n",
    "treino = torch.utils.data.TensorDataset(x_treino_torch, y_treino_torch)\n",
    "validacao = torch.utils.data.TensorDataset(x_valido_torch, y_valido_torch)\n",
    "\n",
    "# Preparamos o dataser para ser iterado pela rede\n",
    "treino_loader = DataLoader(treino, batch_size=tamanho_batch, shuffle=False)\n",
    "validacao_loader = DataLoader(validacao, batch_size=tamanho_batch, shuffle=False)"
   ]
  },
  {
   "source": [
    "### Para que serviu o TensorDataset e o DataLoader?\n",
    "\n",
    "Basicamente, o que eles fazem é transformar nosso conjunto de tensores (que antes eram arrays numpy) em algo iterável, ou seja, que podemos percorrer por com um loop, além disso, já os dividimos em levas (*batchs*) de 100 serão alimentados na nossa rede. Vamos dar uma olhada em como está estruturado nosso `treino_loader`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "limite = 5\n",
    "contador = 0\n",
    "for i in treino_loader:\n",
    "    if contador < limite: print(i)\n",
    "    else: break\n",
    "    contador += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(treino_loader)"
   ]
  },
  {
   "source": [
    "Aqui podemos observar que nosso treino_loader é composto de 500 pares de tensores, vamos olhar melhor cada um desses tensores:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "limite = 5\n",
    "contador = 0\n",
    "for i in treino_loader:\n",
    "    if contador < limite:\n",
    "        print(f\"Tensor de imagens: {i[0]}\\n\"\n",
    "            f\"Tamanho do Tensor: {len(i[0])}\\n\"\n",
    "            f\"Imagem tensor: {i[0][0]}\\n\"\n",
    "            f\"Tamanho da imagem tensor: {len(i[0][0])}\\n\"\n",
    "            f\"------------------------------------------\")\n",
    "    else: break"
   ]
  },
  {
   "source": [
    "Aqui pode-se notar que o primeiro tensor do par é um tensor em que cada elemento é um batch de 100 tensores de imagens. Vamos olhar o segundo elemento do par:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "limite = 5\n",
    "contador = 0\n",
    "for i in treino_loader:\n",
    "    if contador < limite:\n",
    "        print(f\"Tensor de labels: {i[1]}\\n\"\n",
    "            f\"Tamanho do Tensor: {len(i[1])}\\n\"\n",
    "            f\"Label: {i[1][0]}\\n\"\n",
    "            f\"------------------------------------------\")\n",
    "    else: break"
   ]
  },
  {
   "source": [
    "Aqui pode-se notar que o segundo tensor do par é um tensor em que cada elemento é um batch de 100 classificações."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Começando com uma regressão logística\n",
    "\n",
    "Dizemos que uma regressão linear é basicamente uma maneira de visualizar nossos dados em uma \"linha\" e que a partir dessa linha podemos fazer algumas predições sobre dados futuros.\n",
    "\n",
    "Porém, regressões lineares não são muito boas com classificações, para isso utilizaremos uma regressão logística.\n",
    "\n",
    "Uma regressão logística é uma regressão linear que utiliza a função **softmax** como uma **função de ativação**:\n",
    "$$\n",
    "a_i = \\frac{e^{x+i}}{\\sum_{j=1}^n e^{x_j}}\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "> **Obs:** O que a função **softmax** basicamente faz é receber um vetor (lista) de valores numéricos e os transforma em valores probabilísticos. Em outras palavras, quanto maior for o valor da preferência daquela patâmetro, depois que essa lista de valores passar pela função Softmax, maior será sua probabilidade."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "É interessante primeiro estudarmos a regressão logística antes de vermos uma rede neural pois a regressão logística é basicamente uma rede neural simples!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\"<img src=\"./imgs/diagrama_logreg.png\"/>\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressaoLogistica(nn.Module):\n",
    "    def __init__ (self, input_dim, output_dim):\n",
    "        super(RegressaoLogistica, self).__init__()  # Já que será \"filho\" de nn.Module, temos que iniciar seu \"pai\"\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim), # Parte linear\n",
    "            nn.LogSoftmax()                   # A parte logística\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "\n",
    "erro = nn.NLLLoss() # Função de perda (loss)\n",
    "\n",
    "# Complete alguns dos dados faltantes:\n",
    "input_dim = \"...\" # Dica, olhe na especificação do problema\n",
    "output_dim = \"...\"\n",
    "\n",
    "# Inicialize a Regressão Logística com as dimensões de input e output estabelecidas\n",
    "modelo = \"...\"\n",
    "\n",
    "# Aqui escolhemos nosso optimizador, que nesse caso será o Stochastic gradient descent, \n",
    "# que estará optimizando os parâmetros de nossa regrssão linear.\n",
    "taxa_aprendizado = 0.001\n",
    "optmizador = torch.optim.SGD(modelo.parameters(), lr=taxa_aprendizado)"
   ]
  },
  {
   "source": [
    "### Treinando nosso modelo:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contagem = 0\n",
    "lista_loss = []\n",
    "lista_iteracao = []\n",
    "for epoch in range(n_epochs):\n",
    "    for imagens, classificacao in treino_loader:\n",
    "        \n",
    "        # Utilizando a função Variable() crie as seguintes variáveis:\n",
    "        treino = \"...\"                          # Crie uma variável com as imagens, porém, mude a forma do tensor\n",
    "                                                # para ([100,28*28]) com o método .view()\n",
    "        \n",
    "        validacao = \"...\"                       # Crie uma variável com a classificacao\n",
    "\n",
    "        # Limpamos os gradientes\n",
    "        optmizador.zero_grad()\n",
    "\n",
    "        # Utilize o método .forward() do modelo utilizando a variável de treino\n",
    "        # para realizarmos a forward propagation\n",
    "        outputs = \"...\"\n",
    "\n",
    "        # Utilize a função de perda erro() com nosso output e a variável de verificação\n",
    "        loss = \"...\"\n",
    "\n",
    "        # Realizamos a backward propagation\n",
    "        ...\n",
    "\n",
    "        # Atualiza os parâmetros\n",
    "        optmizador.step() \n",
    "\n",
    "        contagem += 1\n",
    "\n",
    "        # Predições\n",
    "        if contagem % 50 == 0:\n",
    "            # Calculamos a acurácia\n",
    "            corretos = 0\n",
    "            total = 0\n",
    "\n",
    "            for imagens, classificacao in treino_loader:\n",
    "                \n",
    "                # Crie uma variável com as imagens da mesma maneira como na variável de treino\n",
    "                teste = \"...\"\n",
    "\n",
    "                # Forward propagation\n",
    "                outputs = \"...\"\n",
    "                \n",
    "                # Recebe as predições do valor máximo\n",
    "                predito =  torch.max(outputs.data, 1)[1]\n",
    "\n",
    "                # Número total de classificações\n",
    "                total += len(classificacao)\n",
    "\n",
    "                # Número de predições corretas\n",
    "                corretos += (predito == classificacao).sum()\n",
    "\n",
    "            acuracia = 100 * corretos / float(total)\n",
    "\n",
    "            # Armazena a loss e iteração\n",
    "            lista_loss.append(loss.data)\n",
    "            lista_iteracao.append(contagem)\n",
    "\n",
    "        if contagem % 500 == 0:\n",
    "            # Printa a loss\n",
    "            print(f\"Iteração: {contagem} | Loss: {loss.data} | Acurácia: {acuracia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,6))\n",
    "plt.plot(lista_iteracao,lista_loss)\n",
    "plt.xlabel(\"Número de Iterações\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Regressão Logística: Loss vs Número de Iterações\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}