{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('torch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a5cd74ba85a3b6a037c59ac3f3634fcdd9437555c9fe253dd51f04000fcd493e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Workshop Redes Neurais\n",
    "## Grupo Turing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Proposta de roteiro para o nb (deletar depois)\n",
    "\n",
    "#### O que deve ser falado antes do nb?\n",
    "- Introdução à redes neurais\n",
    "- estrutura geral de uma rede neural (talvez falar que é parecido com uma regressão logística?)\n",
    "- computation graph (talvez?)\n",
    "- forward propagation\n",
    "- back propagation (se falar de fp acho que é a sequência lógica)\n",
    "- otimizaçaõ e gradient descent\n",
    "\n",
    "#### Conteúdos do nb\n",
    "- Básico de Pytorch\n",
    "  - comparação com np\n",
    "  - operações básicas\n",
    "  - Variables (falar de back propagation?)\n",
    "- Implementando uma NN\n",
    "  - Implementar uma regressão logística (?)\n",
    "  - Implementar NN\n",
    "  - Falar sobre CNN e LSTM (?)\n",
    "  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Básicos de Pytorch\n",
    "\n",
    "Primeiro vamos ver alguns análogos entre **numpy** e **Pytorch**\n",
    "\n",
    "#### Matrizes\n",
    " - Em Pytorch, matrizes (*arrays*) são chamados de tensores.\n",
    " - Uma matriz $3\\times3$, por exemplo é um tensor $3\\times3$\n",
    " - Podemos criar um array numpy com o método `np.numpy()`\n",
    " - Podemos pegar o tipo do array com `type()`\n",
    " - Podemos pegar o formato do *array* com `np.shape()`. Linha $\\times$ Coluna"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Array do tipo: <class 'numpy.ndarray'>\nArray de formato: (2, 3)\n[[1 2 3]\n [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "array = [[1,2,3],[4,5,6]]\n",
    "primeiro_array = np.array(array) # array 2x3\n",
    "print(f\"Array do tipo: {type(primeiro_array)}\")\n",
    "print(f\"Array de formato: {np.shape(primeiro_array)}\")\n",
    "print(primeiro_array)"
   ]
  },
  {
   "source": [
    "- Criamos um tensor com o método `torch.tensor()`\n",
    "- `tensor.type`: tipo do *array*, nesse caso um tensor\n",
    "- `tensor.shape`: formato do *array*. Linha $\\times$ Coluna "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Array do tipo: <built-in method type of Tensor object at 0x7f0179470c80>\nArray de formato: torch.Size([2, 3])\ntensor([[1., 2., 3.],\n        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.Tensor(array)\n",
    "print(f\"Array do tipo: {tensor.type}\")\n",
    "print(f\"Array de formato: {tensor.shape}\")\n",
    "print(tensor)"
   ]
  },
  {
   "source": [
    "Podemos fazer a alocação de *arrays* de maneira análoga nas duas linguages:\n",
    " - `np.ones()` = `torch.ones()`\n",
    " - `np.random.rand()` = `torch.rand()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Numpy:\n [[1. 1. 1.]\n [1. 1. 1.]]\n\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numpy:\\n {np.ones((2,3))}\\n\")\n",
    "\n",
    "print(torch.ones((2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Numpy:\n [[0.63693605 0.22879151 0.9534643 ]\n [0.77120718 0.7379526  0.23432412]]\n\ntensor([[0.9186, 0.2801, 0.6309],\n        [0.9765, 0.9425, 0.5911]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numpy:\\n {np.random.rand(2,3)}\\n\")\n",
    "\n",
    "print(torch.rand(2,3))"
   ]
  },
  {
   "source": [
    "Em muitos pontos **numpy** e **pytorch** são bem parecidos em suas estruturas, e muitas das vezes podemos utilizar os dois em conjunto. Assim normalmente convertemos resultados de redes neurais - que são tensores - para **arrays** de **numpy**.\n",
    "\n",
    "Os métodos para fazer a conversão entre tensores e arrays numpy:\n",
    " - `torch.from_numpy()`: de um array numpy para um tensore\n",
    " - `numpy()`: de um tensor para um array numpy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'> \n [[0.40047898 0.55357586]\n [0.52405586 0.34592202]] \n\ntensor([[0.4005, 0.5536],\n        [0.5241, 0.3459]], dtype=torch.float64) \n\n<class 'numpy.ndarray'> \n [[0.40047898 0.55357586]\n [0.52405586 0.34592202]]\n"
     ]
    }
   ],
   "source": [
    "array = np.random.rand(2,2)\n",
    "print(f\"{type(array)} \\n {array} \\n\")\n",
    "\n",
    "de_numpy_para_tensor = torch.from_numpy(array)\n",
    "print(f\"{de_numpy_para_tensor} \\n\")\n",
    "\n",
    "tensor = de_numpy_para_tensor\n",
    "de_tensor_para_numpy = tensor.numpy()\n",
    "print(f\"{type(de_tensor_para_numpy)} \\n {de_tensor_para_numpy}\")"
   ]
  },
  {
   "source": [
    "### Matemática básica com Pytorch\n",
    "*considere a e b dois tensores*\n",
    "\n",
    "- Redefinir o tamanho: `view()`\n",
    "- Adição: `torch.add(a,b)` = a + b\n",
    "- Subtração: `a.sub(b)` = a - b\n",
    "- Multiplicação elemento-a-elemento = `torch.mul(a,b)` = a * b\n",
    "- Divisão elemento-a-elemento = torch.div(a,b) = a / b\n",
    "- Média: a.mean()\n",
    "- Desvio Padrão (Standart Deviantion - std): a.std()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "torch.Size([9]): tensor([1., 1., 1., 1., 1., 1., 1., 1., 1.]) \n",
      "\n",
      "Adição: \n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]]) \n",
      "\n",
      "Subtração: \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]]) \n",
      "\n",
      "Multiplicação elemento-a-elemento: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Divisão elemento-a-elemento: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Média: 3.0 \n",
      "\n",
      "Desvio padrão: 1.5811388492584229 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(3,3)\n",
    "print(\"\\n\", tensor, \"\\n\")\n",
    "\n",
    "print(f\"{tensor.view(9).shape}: {tensor.view(9)} \\n\")\n",
    "\n",
    "print(f\"Adição: \\n{torch.add(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Subtração: \\n{torch.sub(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Multiplicação elemento-a-elemento: \\n{torch.mul(tensor, tensor)} \\n\")\n",
    "\n",
    "print(f\"Divisão elemento-a-elemento: \\n{torch.div(tensor, tensor)} \\n\")\n",
    "\n",
    "tensor = torch.Tensor([1,2,3,4,5])\n",
    "print(f\"Média: {tensor.mean()} \\n\")\n",
    "\n",
    "print(f\"Desvio padrão: {tensor.std()} \\n\")"
   ]
  },
  {
   "source": [
    "# ATENÇÃO COLOCAR/FAZER EXPLICAÇÃO DE BACKPROP AQUI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Variáveis\n",
    "\n",
    "- Acumulam os gradientes\n",
    "- Na rede neural utilizaremos pytorch. Como explicamos nas redes neurais os gradientes são calculados na *backpropagation*.\n",
    "- A diferença entre variáveis e tensores é a de que variáveis acumulam os gradientes\n",
    "- Também podemos fazer operações matemáticas com variáveis\n",
    "- Dessa maneira, se queremos fazer a *backpropagation* precisamos de variáveis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.], requires_grad=True)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "var = Variable(torch.ones(3), requires_grad = True)\n",
    "var"
   ]
  },
  {
   "source": [
    "Vamos ver um exemplo de como as Variávies são utilizadas em uma *backpropagation*, com duas função $f(y) = \\sum y$, $y(x) = x^2$ e $x = (3,5)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " x = tensor([3., 5.], requires_grad=True)\n f =  34.0\nGradientes: tensor([ 6., 10.])\n"
     ]
    }
   ],
   "source": [
    "array = [3,5]\n",
    "tensor = torch.Tensor(array)\n",
    "x = Variable(tensor, requires_grad = True)\n",
    "y = x**2\n",
    "print(f\" x = {x}\")\n",
    "\n",
    "f = sum(y)\n",
    "print(f\" f =  {f}\")\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(f\"Gradientes: {x.grad}\")"
   ]
  },
  {
   "source": [
    "Vamos explicar passo a passo quais foram as operações feitas pelo Pytorch:\n",
    "- Primeiro ele recebe os elementos do tensor e faz a primeira operação com eles $y_1 = 3^2 = 9$ e $y_2 = 5^2 = 25$\n",
    "- Agora ele soma o tensor, retornando assim um único valor escalar: $\\sum_i y_i = y_1 + y_2 = 9 + 25 = 34$\n",
    "- O gradiente é a derivada parcial de cada elemento, ou seja o gradiente \"1\" é a derivada relativa à $y_1$ e o gradiente \"2\" é relativo à $y_2$ \n",
    "- derivada relativa à $y_1$ é $\\frac{\\partial}{\\partial y_1}(3^2) = 2*3 = 6$\n",
    "- derivada relativa à $y_2$ é $\\frac{\\partial}{\\partial y_2}(5^2) = 2*5 = 10$\n",
    "- Assim ficamos com os gradientes $(6, 10)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}